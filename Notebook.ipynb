{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prototype-based Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -U scikit-fuzzy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Program Files\\Python310\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import shap\n",
    "import pickle\n",
    "from abc import ABC, abstractmethod \n",
    "\n",
    "import skfuzzy as fuzz\n",
    "from scipy import linalg as la\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.base import MultiOutputMixin, BaseEstimator\n",
    "from sklearn.cluster import KMeans, SpectralClustering\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(font_scale=1.6, style='whitegrid')\n",
    "import skfuzzy as fuzz\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".output_wrapper, .output {\n",
       "    height:auto !important;\n",
       "    max-height:10000px;  /* your desired max-height here */\n",
       "}\n",
       ".output_scroll {\n",
       "    box-shadow:none !important;\n",
       "    webkit-box-shadow:none !important;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    ".output_wrapper, .output {\n",
    "    height:auto !important;\n",
    "    max-height:10000px;  /* your desired max-height here */\n",
    "}\n",
    ".output_scroll {\n",
    "    box-shadow:none !important;\n",
    "    webkit-box-shadow:none !important;\n",
    "}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ClustererWrapper super class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClustererWrapper(MultiOutputMixin, BaseEstimator, ABC):\n",
    "\n",
    "    def __init__(self, n_centers=3, seed=0):\n",
    "        \n",
    "        self.n_centers = n_centers\n",
    "        self.centroids = None\n",
    "        self.seed = seed\n",
    "    \n",
    "    @abstractmethod\n",
    "    def fit(self, X_train, Y_train=None):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def predict(self, X):\n",
    "        pass\n",
    "\n",
    "    def f_importance(self, X):\n",
    "\n",
    "        f_values = np.zeros(X.shape[1])\n",
    "        for xi in enumerate(self.centroids):\n",
    "            for xj in enumerate(self.centroids):\n",
    "                f_values += np.abs(xi[1]-xj[1])\n",
    "        \n",
    "        return f_values/np.sum(f_values)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fuzzy c-means wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FuzzyCMeansWrapper(ClustererWrapper):\n",
    "\n",
    "    def __init__(self, n_centers=3, seed=0):\n",
    "           \n",
    "        super().__init__(n_centers, seed)\n",
    "  \n",
    "    def fit(self, X_train, Y_train=None):\n",
    "        \n",
    "        self.centroids, _, _, _, _, _, _ = fuzz.cluster.cmeans(\n",
    "        X_train.T, self.n_centers, m=2, error=0.005, maxiter=1000, seed=self.seed)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "\n",
    "        u, _, _, _, _, _ = fuzz.cluster.cmeans_predict(\n",
    "            X.T, self.centroids, m=2, error=0.005, maxiter=1000, seed=self.seed)\n",
    "\n",
    "        return u.T\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-means wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KMeansWrapper(ClustererWrapper):\n",
    "\n",
    "    def __init__(self, n_centers=3, seed=0):\n",
    "\n",
    "        super().__init__(n_centers, seed)\n",
    "        self.kmeans = KMeans(self.n_centers, random_state=self.seed)\n",
    "    \n",
    "    def fit(self, X_train, Y_train=None):\n",
    "        \n",
    "        self.kmeans = self.kmeans.fit(X_train)\n",
    "        self.centroids = self.kmeans.cluster_centers_\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "\n",
    "        X = np.array(X)\n",
    "        prediction_matrix = np.zeros((len(X), len(self.centroids)))\n",
    "\n",
    "        cluster_labels = self.kmeans.predict(X)\n",
    "        for i, label in enumerate(cluster_labels):\n",
    "            prediction_matrix[i, label] += 1\n",
    "            \n",
    "        return prediction_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spectral clustering wrapper\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Note:```\n",
    "Not sure if the method really makes sense for Spectral Clustering, as the method assumes that the clusters are spherical.\n",
    "\n",
    "Another problem is that Sklearn's SpectralClustering class expects at least 2 point to perform the clustering. It would seem that ``shap.shap_values()`` function feeds the prediction function one sample at a time. \n",
    "\n",
    "Thus, the following error is thrown:  \n",
    "<img src=\"spectral_clustering_error.png\" width=\"100%\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpectralClusteringWrapper(ClustererWrapper):\n",
    "\n",
    "    def __init__(self, n_centers=3, seed=0):\n",
    "\n",
    "        super().__init__(n_centers, seed)\n",
    "        self.spectral_clustering = SpectralClustering(self.n_centers, random_state=self.seed)\n",
    "\n",
    "    def compute_centroids(self, X):\n",
    "        \n",
    "        centroids = np.zeros(self.n_centers)\n",
    "        for k in range(self.n_centers):\n",
    "            centroids[k] = np.mean([x for i, x in enumerate(X) if self.spectral_clustering.labels_[i] == k])\n",
    "\n",
    "        return centroids\n",
    "\n",
    "    def fit(self, X_train, Y_train=None):\n",
    "\n",
    "        X_train = np.array(X_train)\n",
    "        \n",
    "        self.spectral_clustering = self.spectral_clustering.fit(X_train)\n",
    "        self.centroids = self.compute_centroids(X_train)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        \n",
    "        X = np.array(X)\n",
    "        prediction_matrix = np.zeros((len(X), len(self.centroids)))\n",
    "\n",
    "        cluster_labels = self.spectral_clustering.fit_predict(X)\n",
    "        for i, label in enumerate(cluster_labels):\n",
    "            prediction_matrix[i, label] += 1\n",
    "            \n",
    "        return prediction_matrix\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulation Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline(clustering_algorithm, df, dataset_name, n_centers):\n",
    "\n",
    "    ALGORITHM_NAME = clustering_algorithm.__name__.replace('Wrapper', '')\n",
    "\n",
    "    # computing the clustering ground truth\n",
    "    clustering_algo = clustering_algorithm(n_centers, seed=0).fit(df.values)\n",
    "    temp = clustering_algo.predict(df.values)\n",
    "    y_true = np.argmax(clustering_algo.predict(df.values), axis=1)\n",
    "\n",
    "\n",
    "    try:\n",
    "        # load previously computed SHAP values if available\n",
    "        shap_values = pickle.load(open('shap_values/'+dataset_name+'_'+ALGORITHM_NAME+'_shap.dat','rb'))\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        # compute the SHAP values from SCRATCH\n",
    "        explained_model = shap.KernelExplainer(clustering_algo.predict, df)\n",
    "        shap_values = explained_model.shap_values(df)\n",
    "        with open('shap_values/'+dataset_name+'_'+ALGORITHM_NAME+'_shap.dat', 'wb') as file:\n",
    "            pickle.dump(shap_values, file)\n",
    "\n",
    "    labels = ['c'+str(i) for i in range(1,n_centers+1)]\n",
    "    shap.summary_plot(shap_values=shap_values, features=df.columns, feature_names=None, \n",
    "                  plot_type='bar', class_names=labels, color=plt.get_cmap(\"tab20c\"), show=False)\n",
    "\n",
    "    plt.xlabel(\"SHAP value\", fontsize=18)\n",
    "    plt.tick_params(axis='x', labelsize=18)\n",
    "    plt.tick_params(axis='y', labelsize=18)\n",
    "    plt.gca().patch.set_edgecolor('lightgrey')  \n",
    "    plt.gca().patch.set_linewidth(1)\n",
    "    plt.legend(fontsize=18)\n",
    "    plt.title(f'Dataset: {dataset_name}      Algorithm: {ALGORITHM_NAME}', size=12)\n",
    "    plt.savefig('shap_values/'+dataset_name+'_'+ALGORITHM_NAME+'_figure.pdf', bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "    # computing the average SHAP values\n",
    "    ave_shap_values = np.zeros(shap_values[0].shape[1])\n",
    "    for shap_i in shap_values:\n",
    "        ave_shap_values += np.mean(np.absolute(shap_i), axis=0)\n",
    "\n",
    "    # sorting the features by their SHAP value\n",
    "    shap_df = pd.DataFrame(columns=['features','shap'])\n",
    "    shap_df['features'] = df.columns\n",
    "    shap_df['shap'] = ave_shap_values\n",
    "    shap_df.sort_values(by='shap', axis=0, ascending=False, inplace=True)\n",
    "    shap_labels = shap_df['features'].tolist()\n",
    "\n",
    "    errors_shap = [1.0]\n",
    "    exclude = []\n",
    "\n",
    "    # computing the perturbation errors for SHAP\n",
    "    for fi in shap_labels:\n",
    "\n",
    "        df_temp = df.copy()\n",
    "        exclude.append(fi)\n",
    "\n",
    "        for column in exclude:\n",
    "            df_temp[column] = df_temp[column].mean()\n",
    "\n",
    "        y_pred = np.argmax(clustering_algo.predict(df_temp.values), axis=1)\n",
    "        errors_shap.append(accuracy_score(y_true, y_pred))\n",
    "    \n",
    "    # computing the results of the PBFI method\n",
    "    pbfi_df = pd.DataFrame(columns=['features','pfi'])\n",
    "    pbfi_df['features'] = df.columns\n",
    "    pbfi_df['pfi'] = clustering_algo.f_importance(df.values)\n",
    "    pbfi_df.sort_values(by='pfi', axis=0, ascending=False, inplace=True)\n",
    "    pbfi_labels = pbfi_df['features'].tolist()\n",
    "\n",
    "    errors_pbfi = [1.0]\n",
    "    exclude = []\n",
    "\n",
    "    # computing the perturbation errors for PFI\n",
    "    for fi in pbfi_labels:\n",
    "\n",
    "        df_temp = df.copy()\n",
    "        exclude.append(fi)\n",
    "\n",
    "        for column in exclude:\n",
    "            df_temp[column] = df_temp[column].mean()\n",
    "\n",
    "        y_pred = np.argmax(clustering_algo.predict(df_temp.values), axis=1)\n",
    "        errors_pbfi.append(accuracy_score(y_true, y_pred))\n",
    "\n",
    "\n",
    "    sns.lineplot(x = range(1,len(shap_labels)+2), y=errors_shap, marker='o', markersize=10, label = \"SHAP\")\n",
    "    sns.lineplot(x = range(1,len(pbfi_labels)+2), y=errors_pbfi, marker='D', markersize=10, label = \"PBFI\")\n",
    "    plt.tick_params(axis='x', labelsize=18)\n",
    "    plt.tick_params(axis='y', labelsize=18)\n",
    "    plt.xlabel('rank', fontsize=18)\n",
    "    plt.ylabel('accuracy', fontsize=18)\n",
    "\n",
    "    plt.gca().fill_between(range(1,len(shap_labels)+2), errors_shap, errors_pbfi, alpha=0.2, color='grey')\n",
    "\n",
    "    def label_points(shap_labels, errors_shap, pbfi_labels, errors_pbfi, ax):\n",
    "        x = range(1,len(shap_labels)+2)\n",
    "        shap_points = pd.DataFrame({'x': x, 'y': errors_shap, 'label': [''] + shap_labels})\n",
    "        pbfi_points = pd.DataFrame({'x': x, 'y': errors_pbfi, 'label': [''] + pbfi_labels})\n",
    "\n",
    "        for point_shap, point_pbfi in zip(shap_points.iterrows(), pbfi_points.iterrows()):\n",
    "\n",
    "            if point_shap[1]['y'] != point_pbfi[1]['y']:\n",
    "                ax.text(point_shap[1]['x']+.02, point_shap[1]['y']+.02, str(point_shap[1]['label']), size=11)\n",
    "                ax.text(point_pbfi[1]['x']+.02, point_pbfi[1]['y']+.02, str(point_pbfi[1]['label']), size=11)\n",
    "            elif point_shap[1]['label'] != point_pbfi[1]['label']:\n",
    "                ax.text(point_shap[1]['x']+.02, point_shap[1]['y']+.06, str(point_shap[1]['label']), size=11, color='#1f77b4')\n",
    "                ax.text(point_pbfi[1]['x']+.02, point_pbfi[1]['y']+.02, str(point_pbfi[1]['label']), size=11, color='#ff7f0e')\n",
    "            else:\n",
    "                ax.text(point_shap[1]['x']+.02, point_shap[1]['y']+.02, str(point_shap[1]['label']), size=11)\n",
    "            \n",
    "\n",
    "    label_points(shap_labels, errors_shap, pbfi_labels, errors_pbfi, plt.gca())\n",
    "    plt.title(f'Dataset: {dataset_name}      Algorithm: {ALGORITHM_NAME}', size=12)\n",
    "    plt.savefig('feature_importance/error_'+dataset_name+'_'+ALGORITHM_NAME+'.pdf', bbox_inches='tight')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = ['ecoli', 'glass', 'heart-statlog', 'iris', 'liver-disorders', 'pima', 'vehicle', \n",
    "            'wine-quality-red', 'yeast', 'vertebra-column-2c', 'saheart', 'new-thyroid',\n",
    "            'echocardiogram', 'appendicitis', 'hayes-roth']\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "paper_rc = {'lines.linewidth': 1, 'lines.markersize': 7} \n",
    "sns.set_context('paper', font_scale=1.8, rc=paper_rc)\n",
    "\n",
    "for dataset in datasets:\n",
    "    \n",
    "    # loading the current dataset\n",
    "    df = pd.read_csv('datasets/'+dataset+'.csv')\n",
    "    n_centers = len(np.unique(df.values[:,-1]))\n",
    "    df = df.drop(df.columns[-1], axis='columns')\n",
    "\n",
    "    for clustering_algorithm in [FuzzyCMeansWrapper, KMeansWrapper]:#, SpectralClusteringWrapper]:\n",
    "\n",
    "        pipeline(clustering_algorithm=clustering_algorithm, df=df, dataset_name=dataset, n_centers=n_centers)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
