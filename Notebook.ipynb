{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prototype-based Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -U scikit-fuzzy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import shap\n",
    "import pickle\n",
    "from abc import ABC, abstractmethod \n",
    "\n",
    "import skfuzzy as fuzz\n",
    "from scipy import linalg as la\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.base import MultiOutputMixin, BaseEstimator\n",
    "from sklearn.cluster import KMeans, SpectralClustering\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(font_scale=1.6, style='whitegrid')\n",
    "import skfuzzy as fuzz\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".output_wrapper, .output {\n",
       "    height:auto !important;\n",
       "    max-height:10000px;  /* your desired max-height here */\n",
       "}\n",
       ".output_scroll {\n",
       "    box-shadow:none !important;\n",
       "    webkit-box-shadow:none !important;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    ".output_wrapper, .output {\n",
    "    height:auto !important;\n",
    "    max-height:10000px;  /* your desired max-height here */\n",
    "}\n",
    ".output_scroll {\n",
    "    box-shadow:none !important;\n",
    "    webkit-box-shadow:none !important;\n",
    "}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ClustererWrapper super class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClustererWrapper(MultiOutputMixin, BaseEstimator, ABC):\n",
    "\n",
    "    def __init__(self, n_centers=3, seed=0):\n",
    "        \n",
    "        self.n_centers = n_centers\n",
    "        self.centroids = None\n",
    "        self.seed = seed\n",
    "    \n",
    "    @abstractmethod\n",
    "    def fit(self, X_train, Y_train=None):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def predict(self, X):\n",
    "        pass\n",
    "\n",
    "    def f_importance(self, X):\n",
    "\n",
    "        f_values = np.zeros(X.shape[1])\n",
    "        for xi in enumerate(self.centroids):\n",
    "            for xj in enumerate(self.centroids):\n",
    "                f_values += np.abs(xi[1]-xj[1])\n",
    "        \n",
    "        return f_values/np.sum(f_values)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fuzzy c-means wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FuzzyCMeansWrapper(ClustererWrapper):\n",
    "\n",
    "    def __init__(self, n_centers=3, seed=0):\n",
    "           \n",
    "        super().__init__(n_centers, seed)\n",
    "  \n",
    "    def fit(self, X_train, Y_train=None):\n",
    "        \n",
    "        self.centroids, _, _, _, _, _, _ = fuzz.cluster.cmeans(\n",
    "        X_train.T, self.n_centers, m=2, error=0.005, maxiter=1000, seed=self.seed)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "\n",
    "        u, _, _, _, _, _ = fuzz.cluster.cmeans_predict(\n",
    "            X.T, self.centroids, m=2, error=0.005, maxiter=1000, seed=self.seed)\n",
    "\n",
    "        return u.T\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-means wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KMeansWrapper(ClustererWrapper):\n",
    "\n",
    "    def __init__(self, n_centers=3, seed=0):\n",
    "\n",
    "        super().__init__(n_centers, seed)\n",
    "        self.kmeans = KMeans(self.n_centers, random_state=self.seed)\n",
    "    \n",
    "    def fit(self, X_train, Y_train=None):\n",
    "        \n",
    "        self.kmeans = self.kmeans.fit(X_train)\n",
    "        self.centroids = self.kmeans.cluster_centers_\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "\n",
    "        X = np.array(X)\n",
    "        prediction_matrix = np.zeros((len(X), len(self.centroids)))\n",
    "\n",
    "        cluster_labels = self.kmeans.predict(X)\n",
    "        for i, label in enumerate(cluster_labels):\n",
    "            prediction_matrix[i, label] += 1\n",
    "            \n",
    "        return prediction_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spectral clustering wrapper\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Note:```\n",
    "Not sure if the method really makes sense for Spectral Clustering, as the method assumes that the clusters are spherical.\n",
    "\n",
    "Another problem is that Sklearn's SpectralClustering class expects at least 2 point to perform the clustering. It would seem that ``shap.shap_values()`` function feeds the prediction function one sample at a time. \n",
    "\n",
    "Thus, the following error is thrown:  \n",
    "<img src=\"spectral_clustering_error.png\" width=\"100%\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpectralClusteringWrapper(ClustererWrapper):\n",
    "\n",
    "    def __init__(self, n_centers=3, seed=0):\n",
    "\n",
    "        super().__init__(n_centers, seed)\n",
    "        self.spectral_clustering = SpectralClustering(self.n_centers, random_state=self.seed)\n",
    "\n",
    "    def compute_centroids(self, X):\n",
    "        \n",
    "        centroids = np.zeros(self.n_centers)\n",
    "        for k in range(self.n_centers):\n",
    "            centroids[k] = np.mean([x for i, x in enumerate(X) if self.spectral_clustering.labels_[i] == k])\n",
    "\n",
    "        return centroids\n",
    "\n",
    "    def fit(self, X_train, Y_train=None):\n",
    "\n",
    "        X_train = np.array(X_train)\n",
    "        \n",
    "        self.spectral_clustering = self.spectral_clustering.fit(X_train)\n",
    "        self.centroids = self.compute_centroids(X_train)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        \n",
    "        X = np.array(X)\n",
    "        prediction_matrix = np.zeros((len(X), len(self.centroids)))\n",
    "\n",
    "        cluster_labels = self.spectral_clustering.fit_predict(X)\n",
    "        for i, label in enumerate(cluster_labels):\n",
    "            prediction_matrix[i, label] += 1\n",
    "            \n",
    "        return prediction_matrix\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulation Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline(clustering_algorithm, df, dataset_name, n_centers):\n",
    "\n",
    "    ALGORITHM_NAME = clustering_algorithm.__name__.replace('Wrapper', '')\n",
    "\n",
    "    # computing the clustering ground truth\n",
    "    clustering_algo = clustering_algorithm(n_centers, seed=0).fit(df.values)\n",
    "    temp = clustering_algo.predict(df.values)\n",
    "    y_true = np.argmax(clustering_algo.predict(df.values), axis=1)\n",
    "\n",
    "\n",
    "    try:\n",
    "        # load previously computed SHAP values if available\n",
    "        shap_values = pickle.load(open('shap_values/'+dataset_name+'_'+ALGORITHM_NAME+'_shap.dat','rb'))\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        # compute the SHAP values from SCRATCH\n",
    "        explained_model = shap.KernelExplainer(clustering_algo.predict, df)\n",
    "        shap_values = explained_model.shap_values(df)\n",
    "        with open('shap_values/'+dataset_name+'_'+ALGORITHM_NAME+'_shap.dat', 'wb') as file:\n",
    "            pickle.dump(shap_values, file)\n",
    "\n",
    "    labels = ['c'+str(i) for i in range(1,n_centers+1)]\n",
    "    shap.summary_plot(shap_values=shap_values, features=df.columns, feature_names=None, \n",
    "                  plot_type='bar', class_names=labels, color=plt.get_cmap(\"tab20c\"), show=False)\n",
    "\n",
    "    plt.xlabel(\"SHAP value\", fontsize=18)\n",
    "    plt.tick_params(axis='x', labelsize=18)\n",
    "    plt.tick_params(axis='y', labelsize=18)\n",
    "    plt.gca().patch.set_edgecolor('lightgrey')  \n",
    "    plt.gca().patch.set_linewidth(1)\n",
    "    plt.legend(fontsize=18)\n",
    "    plt.savefig('shap_values/'+dataset_name+'_'+ALGORITHM_NAME+'_figure.pdf', bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "    # computing the average SHAP values\n",
    "    ave_shap_values = np.zeros(shap_values[0].shape[1])\n",
    "    for shap_i in shap_values:\n",
    "        ave_shap_values += np.mean(np.absolute(shap_i), axis=0)\n",
    "\n",
    "    # sorting the features by their SHAP value\n",
    "    shap_df = pd.DataFrame(columns=['features','shap'])\n",
    "    shap_df['features'] = df.columns\n",
    "    shap_df['shap'] = ave_shap_values\n",
    "    shap_df.sort_values(by='shap', axis=0, ascending=False, inplace=True)\n",
    "    shap_labels = shap_df['features'].tolist()\n",
    "\n",
    "    errors_shap = [1.0]\n",
    "    exclude = []\n",
    "\n",
    "    # computing the perturbation errors for SHAP\n",
    "    for fi in shap_labels:\n",
    "\n",
    "        df_temp = df.copy()\n",
    "        exclude.append(fi)\n",
    "\n",
    "        for column in exclude:\n",
    "            df_temp[column] = df_temp[column].mean()\n",
    "\n",
    "        y_pred = np.argmax(clustering_algo.predict(df_temp.values), axis=1)\n",
    "        errors_shap.append(accuracy_score(y_true, y_pred))\n",
    "    \n",
    "    # computing the results of the PBFI method\n",
    "    pbfi_df = pd.DataFrame(columns=['features','pfi'])\n",
    "    pbfi_df['features'] = df.columns\n",
    "    pbfi_df['pfi'] = clustering_algo.f_importance(df.values)\n",
    "    pbfi_df.sort_values(by='pfi', axis=0, ascending=False, inplace=True)\n",
    "    pbfi_labels = pbfi_df['features'].tolist()\n",
    "\n",
    "    errors_pbfi = [1.0]\n",
    "    exclude = []\n",
    "\n",
    "    # computing the perturbation errors for PFI\n",
    "    for fi in pbfi_labels:\n",
    "\n",
    "        df_temp = df.copy()\n",
    "        exclude.append(fi)\n",
    "\n",
    "        for column in exclude:\n",
    "            df_temp[column] = df_temp[column].mean()\n",
    "\n",
    "        y_pred = np.argmax(clustering_algo.predict(df_temp.values), axis=1)\n",
    "        errors_pbfi.append(accuracy_score(y_true, y_pred))\n",
    "\n",
    "\n",
    "    sns.lineplot(x = range(1,len(shap_labels)+2), y=errors_shap, marker='o', markersize=10, label = \"SHAP\");\n",
    "    sns.lineplot(x = range(1,len(pbfi_labels)+2), y=errors_pbfi, marker='D', markersize=10, label = \"PBFI\");\n",
    "    plt.tick_params(axis='x', labelsize=18)\n",
    "    plt.tick_params(axis='y', labelsize=18)\n",
    "    plt.xlabel('rank', fontsize=18)\n",
    "    plt.ylabel('accuracy', fontsize=18)\n",
    "\n",
    "    plt.gca().fill_between(range(1,len(shap_labels)+2), errors_shap, errors_pbfi, alpha=0.2, color='grey')\n",
    "\n",
    "    def label_points(shap_labels, errors_shap, pbfi_labels, errors_pbfi, ax):\n",
    "        x = range(1,len(shap_labels)+2)\n",
    "        shap_points = pd.DataFrame({'x': x, 'y': errors_shap, 'label': [''] + shap_labels})\n",
    "        pbfi_points = pd.DataFrame({'x': x, 'y': errors_pbfi, 'label': [''] + pbfi_labels})\n",
    "\n",
    "        for point_shap, point_pbfi in zip(shap_points.iterrows(), pbfi_points.iterrows()):\n",
    "\n",
    "            if point_shap[1]['y'] != point_pbfi[1]['y']:\n",
    "                ax.text(point_shap[1]['x']+.02, point_shap[1]['y']+.02, str(point_shap[1]['label']), size=11)\n",
    "                ax.text(point_pbfi[1]['x']+.02, point_pbfi[1]['y']+.02, str(point_pbfi[1]['label']), size=11)\n",
    "            elif point_shap[1]['label'] != point_pbfi[1]['label']:\n",
    "                ax.text(point_shap[1]['x']+.02, point_shap[1]['y']+.06, str(point_shap[1]['label']), size=11, color='#1f77b4')\n",
    "                ax.text(point_pbfi[1]['x']+.02, point_pbfi[1]['y']+.02, str(point_pbfi[1]['label']), size=11, color='#ff7f0e')\n",
    "            else:\n",
    "                ax.text(point_shap[1]['x']+.02, point_shap[1]['y']+.02, str(point_shap[1]['label']), size=11)\n",
    "            \n",
    "\n",
    "    label_points(shap_labels, errors_shap, pbfi_labels, errors_pbfi, plt.gca())\n",
    "\n",
    "    plt.savefig('feature_importance/error_'+dataset_name+'_'+ALGORITHM_NAME+'.pdf', bbox_inches='tight')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 846 background data samples could cause slower run times. Consider using shap.sample(data, K) or shap.kmeans(data, K) to summarize the background as K samples.\n",
      "100%|██████████| 846/846 [2:39:28<00:00, 11.31s/it]    \n",
      "Using 846 background data samples could cause slower run times. Consider using shap.sample(data, K) or shap.kmeans(data, K) to summarize the background as K samples.\n",
      "100%|██████████| 846/846 [1:47:21<00:00,  7.61s/it]\n",
      "Using 1599 background data samples could cause slower run times. Consider using shap.sample(data, K) or shap.kmeans(data, K) to summarize the background as K samples.\n",
      "100%|██████████| 1599/1599 [7:17:24<00:00, 16.41s/it]  \n",
      "Using 1599 background data samples could cause slower run times. Consider using shap.sample(data, K) or shap.kmeans(data, K) to summarize the background as K samples.\n",
      "  8%|▊         | 120/1599 [26:16<5:23:49, 13.14s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [113], line 13\u001b[0m, in \u001b[0;36mpipeline\u001b[1;34m(clustering_algorithm, df, dataset_name, n_centers)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     12\u001b[0m     \u001b[39m# load previously computed SHAP values if available\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m     shap_values \u001b[39m=\u001b[39m pickle\u001b[39m.\u001b[39mload(\u001b[39mopen\u001b[39;49m(\u001b[39m'\u001b[39;49m\u001b[39mshap_values/\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m+\u001b[39;49mdataset_name\u001b[39m+\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m_\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m+\u001b[39;49mALGORITHM_NAME\u001b[39m+\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m_shap.dat\u001b[39;49m\u001b[39m'\u001b[39;49m,\u001b[39m'\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m'\u001b[39;49m))\n\u001b[0;32m     15\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mFileNotFoundError\u001b[39;00m:\n\u001b[0;32m     16\u001b[0m     \u001b[39m# compute the SHAP values from SCRATCH\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\interactiveshell.py:282\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    276\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    277\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIPython won\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt let you open fd=\u001b[39m\u001b[39m{\u001b[39;00mfile\u001b[39m}\u001b[39;00m\u001b[39m by default \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    278\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    279\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39myou can use builtins\u001b[39m\u001b[39m'\u001b[39m\u001b[39m open.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    280\u001b[0m     )\n\u001b[1;32m--> 282\u001b[0m \u001b[39mreturn\u001b[39;00m io_open(file, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'shap_values/wine-quality-red_KMeans_shap.dat'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [116], line 18\u001b[0m\n\u001b[0;32m     14\u001b[0m df \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39mdrop(df\u001b[39m.\u001b[39mcolumns[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m], axis\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mcolumns\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     16\u001b[0m \u001b[39mfor\u001b[39;00m clustering_algorithm \u001b[39min\u001b[39;00m [FuzzyCMeansWrapper, KMeansWrapper]:\u001b[39m#, SpectralClusteringWrapper]:\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m     pipeline(clustering_algorithm\u001b[39m=\u001b[39;49mclustering_algorithm, df\u001b[39m=\u001b[39;49mdf, dataset_name\u001b[39m=\u001b[39;49mdataset, n_centers\u001b[39m=\u001b[39;49mn_centers)\n",
      "Cell \u001b[1;32mIn [113], line 18\u001b[0m, in \u001b[0;36mpipeline\u001b[1;34m(clustering_algorithm, df, dataset_name, n_centers)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mFileNotFoundError\u001b[39;00m:\n\u001b[0;32m     16\u001b[0m     \u001b[39m# compute the SHAP values from SCRATCH\u001b[39;00m\n\u001b[0;32m     17\u001b[0m     explained_model \u001b[39m=\u001b[39m shap\u001b[39m.\u001b[39mKernelExplainer(clustering_algo\u001b[39m.\u001b[39mpredict, df)\n\u001b[1;32m---> 18\u001b[0m     shap_values \u001b[39m=\u001b[39m explained_model\u001b[39m.\u001b[39;49mshap_values(df)\n\u001b[0;32m     19\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mshap_values/\u001b[39m\u001b[39m'\u001b[39m\u001b[39m+\u001b[39mdataset_name\u001b[39m+\u001b[39m\u001b[39m'\u001b[39m\u001b[39m_\u001b[39m\u001b[39m'\u001b[39m\u001b[39m+\u001b[39mALGORITHM_NAME\u001b[39m+\u001b[39m\u001b[39m'\u001b[39m\u001b[39m_shap.dat\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m file:\n\u001b[0;32m     20\u001b[0m         pickle\u001b[39m.\u001b[39mdump(shap_values, file)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\shap\\explainers\\_kernel.py:190\u001b[0m, in \u001b[0;36mKernel.shap_values\u001b[1;34m(self, X, **kwargs)\u001b[0m\n\u001b[0;32m    188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkeep_index:\n\u001b[0;32m    189\u001b[0m     data \u001b[39m=\u001b[39m convert_to_instance_with_index(data, column_name, index_value[i:i \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m], index_name)\n\u001b[1;32m--> 190\u001b[0m explanations\u001b[39m.\u001b[39mappend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexplain(data, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs))\n\u001b[0;32m    191\u001b[0m \u001b[39mif\u001b[39;00m kwargs\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mgc_collect\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m    192\u001b[0m     gc\u001b[39m.\u001b[39mcollect()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\shap\\explainers\\_kernel.py:382\u001b[0m, in \u001b[0;36mKernel.explain\u001b[1;34m(self, incoming_instance, **kwargs)\u001b[0m\n\u001b[0;32m    379\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkernelWeights[nfixed_samples:] \u001b[39m*\u001b[39m\u001b[39m=\u001b[39m weight_left \u001b[39m/\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkernelWeights[nfixed_samples:]\u001b[39m.\u001b[39msum()\n\u001b[0;32m    381\u001b[0m \u001b[39m# execute the model on the synthetic samples we have created\u001b[39;00m\n\u001b[1;32m--> 382\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrun()\n\u001b[0;32m    384\u001b[0m \u001b[39m# solve then expand the feature importance (Shapley value) vector to contain the non-varying features\u001b[39;00m\n\u001b[0;32m    385\u001b[0m phi \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros((\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mgroups_size, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mD))\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\shap\\explainers\\_kernel.py:530\u001b[0m, in \u001b[0;36mKernel.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    528\u001b[0m eyVal \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mD)\n\u001b[0;32m    529\u001b[0m \u001b[39mfor\u001b[39;00m j \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mN):\n\u001b[1;32m--> 530\u001b[0m     eyVal \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39my[i \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mN \u001b[39m+\u001b[39m j, :] \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mweights[j]\n\u001b[0;32m    532\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mey[i, :] \u001b[39m=\u001b[39m eyVal\n\u001b[0;32m    533\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnsamplesRun \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "datasets = ['ecoli', 'glass', 'heart-statlog', 'iris', 'liver-disorders', 'pima', 'vehicle', \n",
    "            'yeast', 'vertebra-column-2c', 'saheart', 'new-thyroid',\n",
    "            'echocardiogram', 'appendicitis', 'hayes-roth']\n",
    "\n",
    "'''\n",
    "'wine-quality-red',\n",
    "'''\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "paper_rc = {'lines.linewidth': 1, 'lines.markersize': 7} \n",
    "sns.set_context('paper', font_scale=1.8, rc=paper_rc)\n",
    "\n",
    "for dataset in datasets:\n",
    "    \n",
    "    # loading the current dataset\n",
    "    df = pd.read_csv('datasets/'+dataset+'.csv')\n",
    "    n_centers = len(np.unique(df.values[:,-1]))\n",
    "    df = df.drop(df.columns[-1], axis='columns')\n",
    "\n",
    "    for clustering_algorithm in [FuzzyCMeansWrapper, KMeansWrapper]:#, SpectralClusteringWrapper]:\n",
    "\n",
    "        pipeline(clustering_algorithm=clustering_algorithm, df=df, dataset_name=dataset, n_centers=n_centers)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
